{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed47997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2e5356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\nishi\\anaconda3\\envs\\campusx\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\nishi\\anaconda3\\envs\\campusx\\lib\\site-packages (from gensim) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\nishi\\anaconda3\\envs\\campusx\\lib\\site-packages (from gensim) (1.17.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\nishi\\anaconda3\\envs\\campusx\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\nishi\\anaconda3\\envs\\campusx\\lib\\site-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba89cb",
   "metadata": {},
   "source": [
    "## Phase 3: Deep NLP with LSTM\n",
    "\n",
    "Before:\n",
    "text → TF-IDF / Word2Vec → Logistic Regression\n",
    "\n",
    "Now:\n",
    "text → tokenizer → padded sequences\n",
    "     → embedding layer\n",
    "     → LSTM\n",
    "     → sentiment prediction\n",
    "\n",
    "Here the model:\n",
    "\n",
    "Learns word embeddings automatically\n",
    "Understands word order\n",
    "Captures context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68fb547",
   "metadata": {},
   "source": [
    "## Step 1: Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5507deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d77537",
   "metadata": {},
   "source": [
    "## Step 2: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36ad633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/sentimentdataset.csv\")\n",
    "\n",
    "texts = df[\"Text\"].astype(str)\n",
    "labels = df[\"Sentiment\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436015e3",
   "metadata": {},
   "source": [
    "## Step 3: Convert labels to numbers\n",
    "\n",
    "Neural networks require numeric labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "381da066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf00180",
   "metadata": {},
   "source": [
    "## Step 4: Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a921c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Understand the distribution of sequence lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256f0bd",
   "metadata": {},
   "source": [
    "## Step 5: Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49558b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(sequences, padding='post', maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "803cfe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(732, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291cd8f",
   "metadata": {},
   "source": [
    "## Step 6: Train–test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "821c467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d46944",
   "metadata": {},
   "source": [
    "## Step 7: Build LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26c63158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nishi\\anaconda3\\envs\\campusx\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=20),\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556db666",
   "metadata": {},
   "source": [
    "## Step 8: Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4213de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e2b6e",
   "metadata": {},
   "source": [
    "## Step 9: Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b02d3d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.0462 - loss: 5.6042 - val_accuracy: 0.0612 - val_loss: 5.5432\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.0496 - loss: 5.2998 - val_accuracy: 0.0204 - val_loss: 5.6080\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0530 - loss: 5.0320 - val_accuracy: 0.0680 - val_loss: 5.8492\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0718 - loss: 4.7918 - val_accuracy: 0.0340 - val_loss: 5.5295\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.0650 - loss: 4.6107 - val_accuracy: 0.0748 - val_loss: 5.6924\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0957 - loss: 4.4118 - val_accuracy: 0.0884 - val_loss: 5.6866\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.1162 - loss: 4.2385 - val_accuracy: 0.0884 - val_loss: 5.9491\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.1248 - loss: 4.0483 - val_accuracy: 0.0952 - val_loss: 6.1970\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.1197 - loss: 3.8917 - val_accuracy: 0.0816 - val_loss: 6.2460\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.1231 - loss: 3.7655 - val_accuracy: 0.0748 - val_loss: 6.5591\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed6226",
   "metadata": {},
   "source": [
    "## Why LSTM didn’t improve much\n",
    "\n",
    "This is again because of the dataset limitations, not your model.\n",
    "\n",
    "Main reasons\n",
    "1) Very small dataset\n",
    "\n",
    "Deep learning models need:\n",
    "\n",
    "Thousands or millions of samples\n",
    "\n",
    "Your dataset: ~700 samples\n",
    "\n",
    "So the LSTM:\n",
    "\n",
    "Doesn’t have enough data to learn patterns\n",
    "\n",
    "2) Too many classes\n",
    "\n",
    "You have:\n",
    "\n",
    "Dozens of emotion labels\n",
    "\n",
    "Many classes with only 1–3 samples\n",
    "\n",
    "Deep models struggle with:\n",
    "\n",
    "Small multi-class datasets\n",
    "\n",
    "Severe imbalance\n",
    "\n",
    "But this is actually good for interviews\n",
    "\n",
    "You now have a clear experimental story:\n",
    "\n",
    "Phase 1 worked best on small data.\n",
    "Phase 2 struggled because embeddings need large corpora.\n",
    "Phase 3 showed some improvement but was still limited by dataset size.\n",
    "\n",
    "This shows:\n",
    "\n",
    "Analytical thinking\n",
    "\n",
    "Understanding of model–data relationships\n",
    "\n",
    "Interview-ready explanation\n",
    "\n",
    "If asked:\n",
    "\n",
    "“Why didn’t deep learning perform better?”\n",
    "\n",
    "You can say:\n",
    "\n",
    "“The dataset was very small and highly imbalanced across many sentiment classes. Deep learning models like LSTMs require larger datasets to generalize well, so the performance was limited by data rather than model capability.”\n",
    "\n",
    "That’s a strong, honest answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "campusx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
